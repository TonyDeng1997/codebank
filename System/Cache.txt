In-Process cache vs Distributed Cache

In-process cache: an in-process cache is an object cache built within the same address space as your application. 
The Google Guava Library provides a simple in-process cache API that is a good example.

Distributed cache: is external to your application and quite possibly deployed on multiple nodes 
					forming a large logical cache. 
Memcached is a popular distributed cache. 
Ehcache from Terracotta is a product that can be configured to function either way.
HazelCast.

Consistency

While using an in-process cache, your cache elements are local to a single instance of your application. 
Many medium-to-large applications, however, will not have a single application instance as they will most 
likely be load-balanced. In such a setting, you will end up with as many caches as your application instances, 
each having a different state resulting in inconsistency. State may however be eventually consistent as cached 
items time-out or are evicted from all cache instances.

Distributed caches, although deployed on a cluster of multiple nodes, offer a single logical view (and state) of the cache. 
In most cases, an object stored in a distributed cache cluster will reside on a single node in a distributed cache cluster. 
By means of a hashing algorithm, the cache engine can always determine on which node a particular key-value resides. 
Since there is always a single state of the cache cluster, it is never inconsistent.

Think about the redis server in this case.

Overheads

A distributed cache will have two major overheads that will make it slower than an in-process cache (but better than not caching at all): 
network latency and object serialization

For in-process cache, the overhead is mainly from garbege collection and object timeout and cache size.
To sum up, in a multinode deployment of application, it is always good to have a distributed cache.

Reliability
An in-process cache makes use of the same heap space as your program so one has to be careful 
when determining the upper limits of memory usage for the cache. If your program runs out of memory 
there is no easy way to recover from it.

A distributed cache runs as an independent processes across multiple nodes and therefore failure of a single node does not result in a complete failure of the cache.
As a result of a node failure, items that are no longer cached will make their way into surviving nodes on the next cache miss. Also in the case of distributed caches, the worst consequence of a complete cache failure should be degraded performance of the application as opposed to complete system failure.

ref:
https://dzone.com/articles/process-caching-vs-distributed

